{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Programming in Python  - Group Project Report - Group 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Analytics Graduate Program: MSCI: 6040:0EXA\\0EXF Smr20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Contents:\n",
    "\n",
    "## Folder Structure and contents:\n",
    "\n",
    "| Folder Name | File Name | Description |\n",
    "| :- | :- | :- |\n",
    "| Group7Data | Group7_File1_DataValidation_and_Preprocessing.ipyb | File1 containing Validation and preprocessing steps\n",
    "| Group7Data | Group7_File2_Machine Learning and Prediction.ipynb.ipyb | File2 containing model building steps\n",
    "| Group7Data | Group7_File2_Machine Learning and Prediction.html | HTML File for File1\n",
    "| Group7Data | Group7_File2_Machine Learning and Prediction.html | HTML File for File2\n",
    "| Group7Data | Group7app.py | Flask application\n",
    "| Group7Data > data | Product_data.csv | Product Master Data file \n",
    "| Group7Data > data | Stores_data.csv | Dealer/Customer Master Data file\n",
    "| Group7Data > data | Sales_data.csv | Sales data showing sales by customer by product\n",
    "| Group7Data > models | group7regressionmodel.pkl | Pickle File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File:1 - Data Validation and Preprocessing\n",
    "#### File:2 - Machine Learning and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File 1: Data Validation and Preprocessing\n",
    "\n",
    "# Notebook Structure\n",
    "\n",
    "1. Problem Statement and Objective\n",
    "2. Hypothesis Generation and Data Collection\n",
    "3. Loading Datasets and Libraries\n",
    "4. Understanding and Validating the Data\n",
    "5. Data Exploration - Train, Product, Store \n",
    "6. Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Problem Statement and Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement:** \n",
    "One of the largest office furniture manufacturers in USA produces office furnitures such as desks, seating, storage, workspace products and furniture accessories. These products are sent to a dealer network and then the goods are sold to end customers through stores. Not maintaining the right inventory levels by dealers will result in a sudden influx of orders, where the manufacturer may not able to meet the demand in short lead times. The demand volatility incurred by the dealers results in high business revenue losses to the furniture manufacturer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Objective (Goal):**\n",
    "The main objective of the project is to prevent overstocking and understocking of items by forecasting the demand of furniture items for next week using predictive analytics. Overstocking of furniture goods affects the cash flow of organization and understocking will lead a poor customer experience and led to poor customer loyalty results in missed sales. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hypothesis Generation and Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis generation helps to find the possible view or assertion of an analyst about the problem. To help analyse the problem, the data has been collected to predict the target variable. The input data in multiple CSV files is gathered from the Manufacturer. The column and definitions are in the below order:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Description:**\n",
    "\n",
    "Sales Data - Sales_data.csv \n",
    "- **WEEK_END_DATE** - Week ending date\n",
    "- **STORE_ID** - Store identifier\n",
    "- **PRODUCT_ID** - Product specific identifier\n",
    "- **PRICE** - Selling price of item after discount\n",
    "- **BASE_PRICE** - Base price of item\n",
    "- **DISPLAY** - Product was a part of in-store promotional display\n",
    "- **FEATURE** - Product was in in-store circular/flyer\n",
    "- **UNITS** - Units sold (Target variable in our analysis)\n",
    "\n",
    "Product Data - Product_data.csv\n",
    "- **PRODUCT_ID** - Product specific identifier\n",
    "- **DESCRIPTION**\t- Product description\n",
    "- **MANUFACTURER** - Name of product manufacturer or brand name\n",
    "- **CATEGORY** - Category of product\n",
    "- **SUB_CATEGORY** - sub-category of product\n",
    "- **PRODUCT_WEIGHT_LB** - Product weight measured in LBs\n",
    "\n",
    "Store Data - Store_data.csv\n",
    "- **STORE_ID** - Store number\n",
    "- **STORE_NAME** - Name of store\n",
    "- **ADDRESS_CITY_NAME** - Name of the City\n",
    "- **ADDRESS_STATE_PROV_CODE** - Name of the State\n",
    "- **MSA_CODE** - (Metropolitan Statistical Area) Unique code based on geographic region and population density\n",
    "- **SEG_VALUE_NAME** - Store segment name\n",
    "- **PARKING_SPACE_QTY** - Number of parking spaces in the store parking lot\n",
    "- **SALES_AREA_SIZE_NUM** - Area of store in sqft\n",
    "- **AVG_WEEKLY_ORDERS** - Average weekly orders at a dealer/store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loading Datasets and Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisite Libraries to be installed to setup environment:\n",
    "`pip install seaborn`<br>\n",
    "`pip install pandas`<br>\n",
    "`pip install numpy`<br>\n",
    "`pip install category_encoders`<br>\n",
    "`pip install matplotlib`<br>\n",
    "`pip install DateTime`<br>\n",
    "`pip install seaborn`<br>\n",
    "`pip install sklearn`<br>\n",
    "`pip install statsmodels`<br>\n",
    "`pip install scipy`<br>\n",
    "`pip install flask`<br>\n",
    "`pip install flask_restful`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are provided with three tables containing the required information:\n",
    "\n",
    "- **Product_data**: Consists of details about the product\n",
    "- **Store_data**: Consists of details of various dealers/customers associated with the manufacturer  \n",
    "- **Sales_data**: Contains transaction data of products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data files\n",
    "train = pd.read_csv('data/Sales_data.csv')\n",
    "store_data = pd.read_csv('data/Store_data.csv')\n",
    "product_data = pd.read_csv('data/Product_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the size (rowsxcols) of the dataframes to ensure Size of the csv and the df,\n",
    "train.shape, product_data.shape, store_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Understanding and Validating Data\n",
    "\n",
    "As best coding practice, data validation for each column has to be done . Going through variables one by one to understand what features are available to use in the data sets. The features will be later used to make insights about the data to be able to use it for ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data - Store Data that holds Weekly sales by Dealer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify to make sure data is loaded properly.\n",
    "store_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify to make sure data is loaded properly.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify to make the loaded properly\n",
    "product_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking datatypes of columns in train file\n",
    "train.dtypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "WEEK_END_DATE has the data type object, but its a datetime variable <br>\n",
    "The store number and product codes are read as int, represents categorical variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate Week_end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into the date time format\n",
    "train['WEEK_END_DATE'] = pd.to_datetime(train['WEEK_END_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "train['WEEK_END_DATE'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify data to find any extreme values. The data collected is from July 2017 to April 2020\n",
    "train['WEEK_END_DATE'].min(), train['WEEK_END_DATE'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infereneces:\n",
    "- The data collected is from July 2017 to April 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are any dates missing from this period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time span for which data is available in train df. The last day is not included so the unique weeks will be 141+1\n",
    "(train['WEEK_END_DATE'].max() - train['WEEK_END_DATE'].min())/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The training data is for 142 weeks, based on the number of unique *weekend dates* in the train file. Verify using nnunique function.\n",
    "train['WEEK_END_DATE'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "Every date entry in WEEK_END_DATE represents the last day of the week. Therefore we have 142 weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate STORE_ID and PRODUCT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is there any store and prodcut_id null. Are there any missing values in the variables?\n",
    "train[['STORE_ID', 'PRODUCT_ID']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique values of store_num\n",
    "train['STORE_ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 76 unique stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train['STORE_ID'].value_counts()).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every store has minimum of 1676 transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does each store hold atleast one entry per week?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 76 unique stores and 142 weeks of data for the sales. \n",
    "If each store is selling occupies atleast one row in the data, the minimum number of unique rows should be 142*76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "142*76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['WEEK_END_DATE','STORE_ID']].drop_duplicates().shape\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "Implies that  every store is selling atleast 1 product each week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['PRODUCT_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train['PRODUCT_ID'].value_counts()).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is every product sold in every week. Check using unique values of week_end_date and prodcut_id columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['WEEK_END_DATE','PRODUCT_ID']].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "142*30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "This implies every product is being sold atleast 1 time every week as per the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we have information for the sale of every product that is present in the product table (30), against each store associated (76), and for every week (142); we should have `142*76*30` data rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the cross product of Weeks*product*store\n",
    "142*76*30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is each store selling each product throughout the given period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "232286/323760"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "We can conclude that all stores are not selling all products each week\n",
    "of all the possible combinations, about 72% of the data is present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a store selling a particular product, do we have more than one entry?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each product sold by any store should hold only one row, i.e. a particular store,say 'store A' selling a product 'prod P' should contribute a single row for every week. Validate the theory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['WEEK_END_DATE','STORE_ID','PRODUCT_ID']].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(['WEEK_END_DATE','STORE_ID'])['PRODUCT_ID'].count().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inferences:**\n",
    "- The shape does not change after using drop duplicates,\n",
    "- Implies that there are unique combinations for week, store and Product_id\n",
    "- On an average, each week we are selling 22 products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is a store selling a product throughout the period or is there a break?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train.groupby(['STORE_ID', 'PRODUCT_ID'])['UNITS'].count()).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above result, it is observed that not all stores sell a product throughout the week.\n",
    "Our total week count is 142, here the min count shows 137\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate BASE_PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['BASE_PRICE'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['BASE_PRICE'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of Base Price variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.distplot((train['BASE_PRICE'].values), bins=50, color = \"teal\" , kde=True)\n",
    "plt.xlabel('Price Distribution', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate FEATURE and DISPLAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['FEATURE','DISPLAY']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['FEATURE','DISPLAY']].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['FEATURE','DISPLAY']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['FEATURE'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about 9-10% product was in display in Stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['FEATURE'].value_counts(normalize=True).plot(kind='bar',color='mediumturquoise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 10 percent of product are featured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['DISPLAY'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 13% product was in display in Stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['DISPLAY'].value_counts(normalize=True).plot(kind='bar',color='blueviolet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train['FEATURE'], train['DISPLAY']).apply(lambda r: r/len(train), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate UNITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['UNITS'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic statistical details of UNITS variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['UNITS'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "- The Range of values is very high\n",
    "- Minimum number of units sold is 0 and maximum is 1800 \n",
    "- A huge difference between the 75th percentile and the max value indicates presence of outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate maximum units rows from df. From the describe, max units is 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['UNITS'] == 1800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate minimum units rows from df. From the describe, min units is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['UNITS'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot for UNITS variable\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x = range(train.shape[0]), y = np.sort(train['UNITS'].values),color='chocolate')\n",
    "plt.xlabel('Index', fontsize=12)\n",
    "plt.ylabel('Units Sold', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inferences:**\n",
    "1. Most of the values are less than 250\n",
    "2. There are a few entries that are outliers (with 1 outlier way outside the range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product data - Item master showing features of the item and categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first five rows of product data\n",
    "product_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating PRODUCT_ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data['PRODUCT_ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate if all the product in train data set are in product data set and viceversa**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(product_data.PRODUCT_ID).intersection(set(train.PRODUCT_ID)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "All products in train data set and product data set matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating CATEGORY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number and list of unique categories in the product data\n",
    "product_data['CATEGORY'].nunique(), product_data['CATEGORY'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data['CATEGORY'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data['CATEGORY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate SUB_CATEGORY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data['SUB_CATEGORY'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data['SUB_CATEGORY'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying subcategories against each category\n",
    "product_data[['CATEGORY','SUB_CATEGORY']].drop_duplicates().sort_values(by = 'CATEGORY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Inferences:\n",
    " - The sub-categories give additional detail about the product.\n",
    "    - Accessories has 2 sub-categories (Lighting and Power access)\n",
    "    - Tables has Cafe tables and Height-adjustable table as subcategories\n",
    "    - Seating and Storage have just 1 sub category, no further division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate Brand / MANUFACTURER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data['MANUFACTURER'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data['MANUFACTURER'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the list of manufacturers against the 4 categories\n",
    "temp = product_data[['CATEGORY','MANUFACTURER']].drop_duplicates()\n",
    "pd.crosstab([temp['CATEGORY']], temp['MANUFACTURER'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "We have 4 unique categories of Products (Accessories, Seating, Storage, Tables)<br>\n",
    "Each category is associated with more than 1 brand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Data - Customer master that holds all the dealer/customer details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate STORE_ID and STORE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data['STORE_ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Validate if all stores in the train_data is present in store_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(store_data.STORE_ID).intersection(set(train.STORE_ID)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data['STORE_NAME'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique store name:\n",
    "store_data['STORE_NAME'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of store names repeating\n",
    "store_data['STORE_NAME'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "There are no stores without name in the data set<br>\n",
    "The number of unique store IDs is more than number of unique store names which implies that there might be stores with same name, located in different city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate ADDRESS_CITY_NAME  and ADDRESS_STATE_PROV_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data[['ADDRESS_STATE_PROV_CODE', 'ADDRESS_CITY_NAME']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many cities and states are the stores located in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data[['ADDRESS_STATE_PROV_CODE', 'ADDRESS_CITY_NAME']].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the number of stores in each of the state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data.groupby(['ADDRESS_STATE_PROV_CODE'])['STORE_ID'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "Each store has a unique store ID <br>\n",
    "Most stores are from Ohio and Texas ~93%<br>\n",
    "Few from Kentucky and Indiana ~7%<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data.groupby(['ADDRESS_STATE_PROV_CODE'])['ADDRESS_CITY_NAME'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data['ADDRESS_CITY_NAME'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate MSA_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data['MSA_CODE'].nunique(), store_data['MSA_CODE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data['MSA_CODE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(store_data.groupby(['MSA_CODE', 'ADDRESS_STATE_PROV_CODE'])['STORE_ID'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "These codes are assigned based on the geographical location and population density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate PARKING_SPACE_QTY  and SALES_AREA_SIZE_NUM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data[['PARKING_SPACE_QTY', 'SALES_AREA_SIZE_NUM']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.distplot(store_data['PARKING_SPACE_QTY'], bins=25, kde=False, color='magenta')\n",
    "plt.xlabel('Parking Area Size', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference:\n",
    "Out of the 76 stores, parking area of 51 is missing as per our data set\n",
    "<br>About 20 stores have parking area between 250 - 500 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.distplot(store_data['SALES_AREA_SIZE_NUM'], bins=30, kde=True,color='darkmagenta')\n",
    "plt.xlabel('Sales Area Size (Sq Feet)', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "- Most stores have the area between 30-70 K\n",
    "- Only a small number of stores have area less than 30k or greater than 90k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is Average store size varying for different states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(store_data.groupby(['ADDRESS_STATE_PROV_CODE'])['SALES_AREA_SIZE_NUM'].mean()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_oh = store_data.loc[store_data['ADDRESS_STATE_PROV_CODE'] == 'OH']\n",
    "state_tx = store_data.loc[store_data['ADDRESS_STATE_PROV_CODE'] == 'TX']\n",
    "\n",
    "sns.distplot(state_oh['SALES_AREA_SIZE_NUM'], hist=False,color= 'dodgerblue', label= 'OHIO')\n",
    "sns.distplot(state_tx['SALES_AREA_SIZE_NUM'], hist=False,  color= 'orange', label= 'TEXAS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Indiana has only one store and the area size is 58,563 sq feet. \n",
    "- Ohio and Texas have average around 52k and 50k. \n",
    "- Ohio has stores distributed at all sizes.\n",
    "- Texas mainly has stores between sales area 30k to 60k "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate AVG_WEEKLY_ORDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data['AVG_WEEKLY_ORDERS'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data['AVG_WEEKLY_ORDERS'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.distplot(store_data['AVG_WEEKLY_ORDERS'], bins=30, kde=True,color='gold')\n",
    "plt.xlabel('Average Baskets sold per week', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the average weekly orders sold for the states? This will show if the sales is concentrated to one area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(store_data.groupby(['ADDRESS_STATE_PROV_CODE'])['AVG_WEEKLY_ORDERS'].mean()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate SEG_VALUE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data['SEG_VALUE_NAME'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are certain segments assigned to store, based on the brand and quality of products sold at the store.\n",
    "\n",
    "- **Upscale stores** : Located in high income neighborhoods and offer more high-end product\n",
    "- **Mainstream stores** : Located in middle class areas, offering a mix of upscale and value product\n",
    "- **Value stores** : Focus on low prices products targeting low income customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find out the distribution of stores in each of these segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data['SEG_VALUE_NAME'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does the segment has any relation with the store area? Is there a difference in the average sales for each segment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(store_data.groupby(['SEG_VALUE_NAME'])['SALES_AREA_SIZE_NUM'].mean()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(store_data.groupby(['SEG_VALUE_NAME'])['AVG_WEEKLY_ORDERS'].mean()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "Higher segment value has higher sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Exploration - Sales, Product, Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a part of hypothesis generation, we validated the below variables against the data to understand any trend or pattern on the product sales, if any.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze and identify if there is a trend of a pattern between these variables:**\n",
    "- Merging the Store and Product Datasets\n",
    "- Product sales and Weekend Date (Sales by week)\n",
    "- Category wise Product sales and Week end date - Weekly product sales\n",
    "- Dealer, Season and Product Sales\n",
    "- Featured or Displayed Product and Product sale\n",
    "- Product price and Product Sales \n",
    "- Brand and Product sales\n",
    "- Store location (State) and product sales\n",
    "- Dealer Store Size and Product sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the Store and Product Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_product_data = train.merge(product_data, how = 'left', on='PRODUCT_ID')\n",
    "\n",
    "store_product_data = store_product_data.merge(store_data, how = 'left', on = 'STORE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify\n",
    "store_product_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_product_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product sales and Weekend Date (Sales by week)\n",
    "Is there a trend on  sales or demand patterns over the period of time given in data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum of units sold per week\n",
    "weekly_demand = store_product_data.groupby(['WEEK_END_DATE'])['UNITS'].sum()\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "sns.lineplot(x = weekly_demand.index, y = weekly_demand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "- Displays the total number of units sold by the dealer (including all products and from all stores)\n",
    "- The highest number is close to 80,000 and lowest is close to 20,000 units\n",
    "- There is no evident pattern or trend in the plot\n",
    "- The spikes can be seen in either direction and at no constant interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category wise Product sales and Week end date - Weekly product sales\n",
    "\n",
    "Is there a trend on category wise sales or demand patterns or any similarity within each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot weekly sales of products\n",
    "def product_plots(product_list):\n",
    "    \n",
    "    # dictionary storing UPC and weekly sales\n",
    "    d = {product: store_product_data[store_product_data['PRODUCT_ID'] == product].groupby(['WEEK_END_DATE'])['UNITS'].sum() for product in product_list}\n",
    "    fig, axs = plt.subplots(len(product_list), 1, figsize = (20, 20), dpi=300)\n",
    "    j = 0\n",
    "    \n",
    "    for product in d.keys():\n",
    "        # adding manufacturer and descritption in title\n",
    "        manu = product_data[product_data['PRODUCT_ID'] == product]['MANUFACTURER'].values[0]\n",
    "        desc = product_data[product_data['PRODUCT_ID'] == product]['DESCRIPTION'].values[0]            \n",
    "        # creating the plot\n",
    "        sns.lineplot(x = d[product].index, y = d[product],ax = axs[j]).set_title(str(manu)+str(\" \")+str(desc), y=0.75, fontsize = 16)\n",
    "        j = j+1\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of products based on category\n",
    "seating = list(product_data[product_data['CATEGORY'] == 'Seating']['PRODUCT_ID'])\n",
    "tables = list(product_data[product_data['CATEGORY'] == 'Tables']['PRODUCT_ID'])\n",
    "accessories = list(product_data[product_data['CATEGORY'] == 'Accessories']['PRODUCT_ID']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_plots(accessories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_plots(seating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_plots(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "- No increasing/decreasing trend for the sale of products over time\n",
    "- No seasonal patterns seen on individual product sale\n",
    "- Products by same manufaturer have similar patterns (spikes and drops).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealer, Season and Product Sales\n",
    "Is there a pattern of sales in a store? Eg: Holiday season with more sales or School season showing a increasing trend on sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly selecting 5 store ID\n",
    "stores_plot = random.sample(list(store_data['STORE_ID']), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary with store number as keys\n",
    "# for each store, calculate sum of units sold per week\n",
    "d = {store: train[train['STORE_ID'] == store].groupby(['WEEK_END_DATE'])['UNITS'].sum() for store in stores_plot}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "fig, axs = plt.subplots(5, 1, figsize = (15, 15), dpi=300)\n",
    "j = 0\n",
    "for store in d.keys():\n",
    "    sns.lineplot(x = d[store].index, y = d[store],ax = axs[j])\n",
    "    j = j+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infereneces:\n",
    "For the randomly selected store numbers, we can see that there is no pattern in the plot. The same was repeated for a number of stores and the data showed no increasing or decreasing trend or seasonality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featured or Displayed Product and Product sale\n",
    "\n",
    "If the product is featured at the Dealer, does it have an impact on the sales? For example: Featured Products with attractive offers will have higher sales or Sales will be more for products with in-store promotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featured_plots(product_list):\n",
    "    #dictionary storing Product_id and 'Featured' variable\n",
    "    d_f = {product: 1000*train[train['PRODUCT_ID'] == product].groupby(['WEEK_END_DATE'])['FEATURE'].mean() for product in product_list}\n",
    "    #dictionary storing PRODUCT_ID and Product Sales\n",
    "    d = {product: train[train['PRODUCT_ID'] == product].groupby(['WEEK_END_DATE'])['UNITS'].sum() for product in product_list}\n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(len(product_list), 1, figsize = (20, 20), dpi=300)\n",
    "    j = 0\n",
    "    for product in d.keys():\n",
    "        # Manufacturer name and Descritption in title\n",
    "        manu = product_data[product_data['PRODUCT_ID'] == product]['MANUFACTURER'].values[0]\n",
    "        desc = product_data[product_data['PRODUCT_ID'] == product]['DESCRIPTION'].values[0]\n",
    "        \n",
    "        # plotting featured and sales values\n",
    "        sns.lineplot(x = d_f[product].index, y = d_f[product],ax = axs[j]).set_title(str(manu)+str(\" \")+str(desc), y=0.75, fontsize = 16)\n",
    "        sns.lineplot(x = d[product].index, y = d[product],ax = axs[j]).set_title(str(manu)+str(\" \")+str(desc), y=0.75, fontsize = 16)\n",
    "        j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list_f = list(product_data[product_data['CATEGORY'] == 'Tables']['PRODUCT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_plots(product_list_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infereneces:\n",
    "When the products are featured, the sales increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does the in-store display also have a similar effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_plots(product_list):\n",
    "    d_d = {product: 1000*train[train['PRODUCT_ID'] == product].groupby(['WEEK_END_DATE'])['DISPLAY'].mean() for product in product_list}\n",
    "    d = {product: train[train['PRODUCT_ID'] == product].groupby(['WEEK_END_DATE'])['UNITS'].sum() for product in product_list}\n",
    "    fig, axs = plt.subplots(len(product_list), 1, figsize = (20, 20), dpi=300)\n",
    "    j = 0\n",
    "    for product in d.keys():\n",
    "        manu = product_data[product_data['PRODUCT_ID'] == product]['MANUFACTURER'].values[0]\n",
    "        desc = product_data[product_data['PRODUCT_ID'] == product]['DESCRIPTION'].values[0]\n",
    "        sns.lineplot(x = d[product].index, y = d[product],ax = axs[j]).set_title(str(manu)+str(\" \")+str(desc), y=0.75, fontsize = 16)\n",
    "        sns.lineplot(x = d_d[product].index, y = d_d[product],ax = axs[j]).set_title(str(manu)+str(\" \")+str(desc), y=0.75, fontsize = 16)\n",
    "        j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_plots(product_list_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "- It is evident that product sales are greatly affected by the display.\n",
    "- For products on display, the sales are higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product price and Product Sales \n",
    "\n",
    "Finding whether the price of the product have an effect on sales?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of products based on category\n",
    "\n",
    "product_size_seating = store_product_data.loc[store_product_data['CATEGORY']=='Seating']\n",
    "product_size_tables  = store_product_data.loc[store_product_data['CATEGORY']=='Tables']\n",
    "product_size_accessories = store_product_data.loc[store_product_data['CATEGORY']=='Accessories'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot for base price and sales\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x = (product_size_seating['BASE_PRICE']), y = (product_size_seating['UNITS']))\n",
    "plt.xlabel('BASE_PRICE', fontsize=12)\n",
    "plt.ylabel('UNITS', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot for base price and sales\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x = (product_size_tables['BASE_PRICE']), y = (product_size_tables['UNITS']))\n",
    "plt.xlabel('BASE_PRICE', fontsize=12)\n",
    "plt.ylabel('UNITS', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot for base price and sales\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x = (product_size_accessories['BASE_PRICE']), y = (product_size_accessories['UNITS']))\n",
    "plt.xlabel('BASE_PRICE', fontsize=12)\n",
    "plt.ylabel('UNITS', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "- For seatings, items with lower price show a higher sale. \n",
    "- Tables have higher sale for medium priced items. \n",
    "- Accessories show higher sale in high price cateogries with some outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brand and Product sales\n",
    "\n",
    "Finding if there is a trend of high sales on a particular brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = list(product_data[product_data['CATEGORY'] == 'Tables']['PRODUCT_ID'])\n",
    "seating = list(product_data[product_data['CATEGORY'] == 'Seating']['PRODUCT_ID'])\n",
    "accessories = list(product_data[product_data['CATEGORY'] == 'Accessories']['PRODUCT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "ax = sns.boxplot(x=\"PRODUCT_ID\", y=\"BASE_PRICE\", data=train[train['PRODUCT_ID'].isin(tables)])\n",
    "product_data[product_data['PRODUCT_ID'].isin(tables)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "ax = sns.boxplot(x=\"MANUFACTURER\", y=\"UNITS\", data=store_product_data[store_product_data['PRODUCT_ID'].isin(tables)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "ax = sns.boxplot(x=\"MANUFACTURER\", y=\"UNITS\", data=store_product_data[store_product_data['PRODUCT_ID'].isin(seating)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "ax = sns.boxplot(x=\"MANUFACTURER\", y=\"UNITS\", data=store_product_data[store_product_data['PRODUCT_ID'].isin(accessories)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "- There is no evidence on high sales for particular brand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store location (State) and product sales\n",
    "\n",
    "Finding if there is a significant difference in the product sales for different regions? \n",
    "\n",
    "    - Use Store Location: Is there a trend between a store/dealer location(State) and the number of order/units sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_weekly_sales = store_product_data.groupby(['WEEK_END_DATE','STORE_ID'])['UNITS'].sum().reset_index()\n",
    "\n",
    "grouped_weekly_sales = grouped_weekly_sales.merge(store_data, how = 'left', left_on = 'STORE_ID', right_on = 'STORE_ID')\n",
    "\n",
    "grouped_weekly_sales = grouped_weekly_sales.sort_values(by = 'ADDRESS_STATE_PROV_CODE')\n",
    "\n",
    "state = (store_data[['ADDRESS_STATE_PROV_CODE','STORE_ID']].sort_values(by ='ADDRESS_STATE_PROV_CODE'))['STORE_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,15))\n",
    "\n",
    "ax=sns.boxplot(x=\"STORE_ID\",y=\"UNITS\",data=grouped_weekly_sales, hue ='ADDRESS_STATE_PROV_CODE', order =state)\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "- Mostly the number of units is higher for Ohio (considering individual stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealer Store Size and Product sales\n",
    "\n",
    "Finding whether store size increase product sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_agg_data = train.groupby(['STORE_ID'])['UNITS'].sum().reset_index()\n",
    "merged_store_data = store_data.merge(store_agg_data, how = 'left', left_on = 'STORE_ID', right_on = 'STORE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_oh = merged_store_data.loc[merged_store_data['ADDRESS_STATE_PROV_CODE'] == 'OH']\n",
    "state_tx = merged_store_data.loc[merged_store_data['ADDRESS_STATE_PROV_CODE'] == 'TX']\n",
    "\n",
    "sns.distplot(state_oh['SALES_AREA_SIZE_NUM'], hist=False,color= 'dodgerblue', label= 'OHIO')\n",
    "sns.distplot(state_tx['SALES_AREA_SIZE_NUM'], hist=False,  color= 'orange', label= 'TEXAS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = (state_oh['SALES_AREA_SIZE_NUM']), y = (state_oh['UNITS']))\n",
    "sns.scatterplot(x = (state_tx['SALES_AREA_SIZE_NUM']), y = (state_tx['UNITS']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "- Size of Store: Stores with larger area would have more sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing is a technique that is used to convert the raw data into a clean data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING: CATEGORICAL FEATURES\n",
    "\n",
    "- Find out and impute, if we have missing values in the categorical features.\n",
    "- Remove the features which do not add much information\n",
    "- Choose an encoding scheme to convert categorical feature into numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN DATA SET:  \n",
    "Weekly Sales Data contains the following categorical variables:\n",
    " - STORE_NUM\n",
    " - UPC\n",
    " - FEATURE\n",
    " - DISPLAY\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the null values in the categorical features\n",
    "train[['STORE_ID', 'PRODUCT_ID', 'FEATURE', 'DISPLAY']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "***No Null Values*** \n",
    " -  STORE_ID  - No changes required as it is a key and will be used to merge tables later.\n",
    " -  PRODUCT_ID - No changes required as it is a key and will be used to merge tables later.\n",
    " -  FEATURE    - No Preprocessing Required, as the values are already 0 or 1 in the data set.\n",
    " -  DISPLAY    - No Preprocessing Required, as the values are already 0 or 1 in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create new column `WEEKOFYEAR` to capture the year & week number. This will be useful during prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.WEEK_END_DATE = pd.to_datetime(train.WEEK_END_DATE)\n",
    "train['weeknum']=train['WEEK_END_DATE'].dt.week\n",
    "train['year'] = pd.DatetimeIndex(train['WEEK_END_DATE']).year\n",
    "train['WEEKOFYEAR'] =train[['year','weeknum']].dot([100,1])\n",
    "\n",
    "train= train.drop(columns=['weeknum'])\n",
    "train= train.drop(columns=['year'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRODUCT DATA SET :\n",
    "Product Data has the following categorical variables:\n",
    "- PRODUCT_ID\n",
    "- DESCRIPTION\n",
    "- MANUFACTURER\n",
    "- CATEGORY\n",
    "- SUB_CATEGORY\n",
    "- PRODUCT_WEIGHT_LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the null values in the categorical features\n",
    "product_data[['PRODUCT_ID', 'DESCRIPTION', 'MANUFACTURER', 'CATEGORY', 'SUB_CATEGORY', 'PRODUCT_WEIGHT_LB']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `DESCRIPTION` - In the description, we have category, subcategory and size of the product and these are already present in the other features as well. So, We will drop this feature as it will not add much value to the model.\n",
    "- `MANUFACTURER`, `CATEGORY`, `SUB_CATEGORY`- As, there is no order in the given categories, so we will change this as numerical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the DESCRIPTION FEATURE\n",
    "product_data = product_data.drop(columns= ['DESCRIPTION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STORE DATA SET :\n",
    "Store Data has the following categorical variables:\n",
    "- STORE_ID\n",
    "- STORE_NAME\n",
    "- ADDRESS_CITY_NAME\n",
    "- ADDRESS_STATE_PROV_CODE\n",
    "- MSA_CODE\n",
    "- SEG_VALUE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the null values\n",
    "\n",
    "store_data[['STORE_ID', 'STORE_NAME', 'ADDRESS_CITY_NAME', 'ADDRESS_STATE_PROV_CODE', 'MSA_CODE', 'SEG_VALUE_NAME']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - STORE_ID - No changes required as it is a key and will be used to merge files later.\n",
    "   - STORE_NAME - Since, Out of 76 different stores we have 72 unique store names. Store name contains some location information of the store which we have in the form of address city name and state.\n",
    "   - ADDRESS_CITY_NAME - Since, Out of 76 different stores we have 51 unique address city names, So we will drop this feature due to high cardinality\n",
    "   - ADDRESS_STATE_PROV_CODE, MSA_CODE - As, there is no order in the given categories. So, we can convert this as numerical variable using categorization.\n",
    "   - SEG_VALUE_NAME - Stores segments are divided into 3 categories: upscale, mainstream and value. This field has no major impact on the sales and therefore, we can leave the column as is for now and analyse during model fitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop store name and address\n",
    "store_data = store_data.drop(columns=['STORE_NAME', 'ADDRESS_CITY_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns of the updated data\n",
    "store_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING: NUMERICAL FEATURES\n",
    "\n",
    "- Check and impute the missing values in the numerical features.\n",
    "- Check for the outliers and treat them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN DATA SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEEKLY SALES DATA  has the following numerical features\n",
    "\n",
    "    - BASE_PRICE\n",
    "    - UNITS (Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the null values for the numerical features\n",
    "train[[ 'BASE_PRICE', 'UNITS']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Nulls present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot for UNITS variable\n",
    "# sort the target variable and scatter plot to see if it has some outliers or not.  \n",
    "\n",
    "%matplotlib notebook\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x = range(train.shape[0]), y = np.sort(train['UNITS'].values))\n",
    "plt.xlabel('Index', fontsize=12)\n",
    "plt.ylabel('Units Sold', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the number of data points where units are more than 750, as the plot shows sparesely plotted for those values\n",
    "train['UNITS'][train.UNITS > 750].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "There are a some points above where UNITS are more than 750 and there number is only 21. Considering 750 as the cutoff as only less number of plotting is present above 750.<br>\n",
    "So, we can remove them as there number is only 21 and will not affect the data and these will act as a noise to our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the valures where UNITS are more than 750\n",
    "train = train[~(train.UNITS > 750)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRODUCT DATA SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no numerical features in product data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STORE DATA SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STORE DATA has the following numerical features\n",
    "- PARKING_SPACE_QTY\n",
    "- SALES_AREA_SIZE_NUM\n",
    "- AVG_WEEKLY_ORDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the null values\n",
    "store_data[['PARKING_SPACE_QTY', 'SALES_AREA_SIZE_NUM', 'AVG_WEEKLY_ORDERS']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARKING_SPACE_QTY - Check its correlation with the SALES_AREA_SIZE_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation\n",
    "store_data[['PARKING_SPACE_QTY','SALES_AREA_SIZE_NUM']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "Since the correlation of the PARKING_SPACE_QTY with SALES_AREA_SIZE_NUM is high so we can drop this column as it will not add much value to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column\n",
    "store_data = store_data.drop(columns=['PARKING_SPACE_QTY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE THE UPDATED FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/updated_train_data.csv',index=False)\n",
    "product_data.to_csv('data/updated_product_data.csv',index=False)\n",
    "store_data.to_csv('data/updated_store_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_product = pd.read_csv(\"data/updated_product_data.csv\")\n",
    "updated_store= pd.read_csv(\"data/updated_store_data.csv\")\n",
    "updated_train = pd.read_csv(\"data/updated_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_product.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_store.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging THe three filles t be one file\n",
    "\n",
    "final_file = (pd.merge(updated_product, updated_train, on='PRODUCT_ID'))\n",
    "final_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_final_file = (pd.merge(final_file, updated_store, how = 'left', on = 'STORE_ID'))\n",
    "All_final_file.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_final_file.isnull().any(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime\n",
    "All_final_file.WEEK_END_DATE = pd.to_datetime(All_final_file.WEEK_END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_final_file.to_csv('data/merged_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
